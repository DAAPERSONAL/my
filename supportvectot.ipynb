{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supportvectot",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtoXDkZINQzrYlwSjCxnlB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAAPERSONAL/my/blob/main/supportvectot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "SJWhrCTv_2eS",
        "outputId": "0f4b3afa-2a98-420d-d4ae-6bbe8470fa1f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a165a4ef10e2>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    from nltk\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nlk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "from nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perception_tagger')\n",
        "nltk.download('stopword')\n",
        "from nltk.stem import woedNetLemmatizer\n",
        "from sklearn.preprocessing import labelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.features_extraction.text import TfidVectorizer\n",
        "from sklearn import model_selection,naive_bayes,svm\n",
        "from sklearn.matrics import_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "E8mWQhHQDfB5",
        "outputId": "d61fdc38-612e-41bc-997e-3fd641fb3f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bd3e4b6812bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus = pd.read_csv(r\"corpus.cv\",encording='latin-1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "i-47rZ_YDkzw",
        "outputId": "91a60bed-9b31-4dda-e291-e557c6f608ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-08d76d3d72b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"corpus.cv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencording\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step - a :remove blank row if any\n",
        "Corpus['text'].dropna(inplace=True)\n",
        "Corpus['text']=[entry.lower() for every in corpus['text']]\n",
        "Corpus['text']=[word_tokenize(entry) for entry in the corpus['text']]\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "for index,entry in enumerate(Corpus['text']):\n",
        "  Final_words = []\n",
        "  word_lammatized = wordNetLammatizer()\n",
        "  for word, tag in pos_tag(entry):\n",
        "    if word not in stopwords.words('english') and word.isalpha():\n",
        "      word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "      Final_words.append(words_final)\n",
        "      Corpus.loc[index,'text_final'] = str(final_wors)"
      ],
      "metadata": {
        "id": "mUZ-9TEwEHGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_X, Test_X, Train_Y, Train_Y = model_selection,train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)"
      ],
      "metadata": {
        "id": "oIowjxhiIrD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)\n"
      ],
      "metadata": {
        "id": "jPAHvbNIJlEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tfidf_vect = TfidVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(Corpus['text_final'])\n",
        "Train_x_Tfidf = Tfidf_vect.transform(Train_x)\n",
        "Text_X_Tfidf = Tfidf_vect.transform(test_X)\n",
        "print(Tfidf_vect.vocabulary_)\n"
      ],
      "metadata": {
        "id": "scNrzvZ7KK1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Train_X_Tfidf)"
      ],
      "metadata": {
        "id": "NXqyNXoQLoqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = svm.SVC(c=1.0, kernal='linear', degree=3, gamma = 'auto')\n",
        "SVM.fit(train_X_Tfidf,Train_Y)\n",
        "predicions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "print(\"SVM Accuracy Score ->\",accuracy_score(predictions_SVM, Test_Y)*100)"
      ],
      "metadata": {
        "id": "7a6x_2IHL01A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_4PghYO8PUWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}